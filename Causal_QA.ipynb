{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Causal QA.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hkayesh/causal-qa/blob/master/Causal_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0o0IA13c0ja",
        "colab_type": "text"
      },
      "source": [
        "### Install necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC2UWmmqcT0K",
        "colab_type": "code",
        "outputId": "3f20dc8f-1ab3-4869-a8b9-c4065aa69187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.19)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.35)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.36 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.36)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE5VE5MCV2vk",
        "colab_type": "code",
        "outputId": "ab5f80ad-2064-4539-a9f9-fb36d9b06967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "! pip install torch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEEMbJEQc9ZO",
        "colab_type": "text"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STVgVdf3CLDb",
        "colab_type": "code",
        "outputId": "dbf66091-296c-40da-9bb8-26c743200f8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive \n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
        "from transformers import CamembertForSequenceClassification, CamembertTokenizer\n",
        "from transformers import AlbertForSequenceClassification, AlbertTokenizer\n",
        "\n",
        "from smart_open import smart_open\n",
        "from gensim.summarization.textcleaner import split_sentences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers.optimization import AdamW\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, matthews_corrcoef\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MusFVW8ddFFk",
        "colab_type": "text"
      },
      "source": [
        "### Mount Google Drive \n",
        "(from shamolbit@gmail.com)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xma8G1nZEo5T",
        "colab_type": "code",
        "outputId": "f9584ed5-2498-4756-ee28-1608181a6d5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW0VkR-zOM6q",
        "colab_type": "text"
      },
      "source": [
        "### Define Constant Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSeI1kwzNJr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RANDOM_SEED = 1\n",
        "BATCH_SIZE = 32\n",
        "MAX_SEQ_LEN = 128\n",
        "PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
        "NUM_EPOCHS = 4\n",
        "DATASET_DIR_PATH = 'gdrive/My Drive/Research Data/CausalQA/datasets/'\n",
        "# NEWS_ARTICLES_FILE_PATH = 'gdrive/My Drive/Research Data/CausalQA/signalmedia-1m.jsonl'\n",
        "NEWS_ARTICLES_FILE_PATH = 'gdrive/My Drive/Research Data/CausalQA/causal_pairs_10k_articles.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgi5oO9ofZbE",
        "colab_type": "text"
      },
      "source": [
        "#### Set random seeds for reproducibility "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO7ldT5rexMq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6da7ffdf-a295-4cfe-f4b0-bd7071c5bcef"
      },
      "source": [
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8378d03710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "334u0KG9PU5o",
        "colab_type": "text"
      },
      "source": [
        "### Load Pretraiend model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDGVbX_jfTIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME)\n",
        "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
        "\n",
        "# model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\")\n",
        "# tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# model = CamembertForSequenceClassification.from_pretrained()\n",
        "# tokenizer = CamembertTokenizer.from_pretrained()\n",
        "\n",
        "# model = AlbertForSequenceClassification.from_pretrained()\n",
        "# tokenizer = AlbertTokenizer.from_pretrained()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuUvRC48Onaf",
        "colab_type": "text"
      },
      "source": [
        "### Load Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtLVGwlBGfi7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "230cc268-4c4c-4a8b-ac34-df68dadaf460"
      },
      "source": [
        "causal_pairs_df = pd.read_csv(NEWS_ARTICLES_FILE_PATH, lineterminator='\\n', error_bad_lines=False)\n",
        "print(causal_pairs_df.shape)\n",
        "causal_pairs_df.head()\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7230, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sequence_a</th>\n",
              "      <th>sequence_b</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the postal order was chosen because of its cen...</td>\n",
              "      <td>its central location and its proximity to the ...</td>\n",
              "      <td>the postal order was chosen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>posted on sep 22, 2015\\rif you were a basketba...</td>\n",
              "      <td>you were a basketball fan who was born in the 80s</td>\n",
              "      <td>you were lucky enough to witness the beauty th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>individual dreams and visions have been aborte...</td>\n",
              "      <td>words of death spoken over them</td>\n",
              "      <td>individual dreams and visions have been aborted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>homes and marriages have been destroyed becaus...</td>\n",
              "      <td>poisonous words</td>\n",
              "      <td>homes and marriages have been destroyed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>companies and businesses have fallen because o...</td>\n",
              "      <td>words of envy and jealousy</td>\n",
              "      <td>companies and businesses have fallen</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...                                         sequence_b\n",
              "0  the postal order was chosen because of its cen...  ...                        the postal order was chosen\n",
              "1  posted on sep 22, 2015\\rif you were a basketba...  ...  you were lucky enough to witness the beauty th...\n",
              "2  individual dreams and visions have been aborte...  ...    individual dreams and visions have been aborted\n",
              "3  homes and marriages have been destroyed becaus...  ...            homes and marriages have been destroyed\n",
              "4  companies and businesses have fallen because o...  ...               companies and businesses have fallen\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv6TnvklLXNu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "a8d1f16f-51e6-4088-f6bd-c42975d5260f"
      },
      "source": [
        "train_dataset_causal_df = causal_pairs_df[['sequence_a', 'sequence_b']]\n",
        "train_dataset_causal_df['label'] = 'causal'\n",
        "train_dataset_causal_df.head()\n",
        "train_dataset_causal_df.shape\n",
        "\n",
        "train_dataset_not_causal_df = pd.DataFrame()\n",
        "train_dataset_not_causal_df['sequence_a'] = train_dataset_causal_df['sequence_a'].sample(\n",
        "    train_dataset_causal_df.shape[0], random_state=1).tolist()\n",
        "train_dataset_not_causal_df['sequence_b'] = train_dataset_causal_df['sequence_b'].sample(\n",
        "    train_dataset_causal_df.shape[0], random_state=2).tolist()\n",
        "train_dataset_not_causal_df['label'] = 'not_ causal'\n",
        "\n",
        "train_dataset = pd.concat([train_dataset_causal_df, train_dataset_not_causal_df])\n",
        "print(train_dataset.shape)\n",
        "train_dataset.head()\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14460, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_a</th>\n",
              "      <th>sequence_b</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>its central location and its proximity to the ...</td>\n",
              "      <td>the postal order was chosen</td>\n",
              "      <td>causal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>you were a basketball fan who was born in the 80s</td>\n",
              "      <td>you were lucky enough to witness the beauty th...</td>\n",
              "      <td>causal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>words of death spoken over them</td>\n",
              "      <td>individual dreams and visions have been aborted</td>\n",
              "      <td>causal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>poisonous words</td>\n",
              "      <td>homes and marriages have been destroyed</td>\n",
              "      <td>causal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>words of envy and jealousy</td>\n",
              "      <td>companies and businesses have fallen</td>\n",
              "      <td>causal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          sequence_a  ...   label\n",
              "0  its central location and its proximity to the ...  ...  causal\n",
              "1  you were a basketball fan who was born in the 80s  ...  causal\n",
              "2                    words of death spoken over them  ...  causal\n",
              "3                                    poisonous words  ...  causal\n",
              "4                         words of envy and jealousy  ...  causal\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAc4_f9aPiAK",
        "colab_type": "text"
      },
      "source": [
        "#### Prepare training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Py-ZD9uRWVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_a_list = train_dataset['sequence_a'].tolist()\n",
        "seq_b_list = train_dataset['sequence_b'].tolist()\n",
        "labels = [1 if label=='causal' else 0 for label in train_dataset['label'].tolist()]\n",
        "\n",
        "input_ids = []\n",
        "token_type_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for seq_a, seq_b in zip(seq_a_list, seq_b_list):\n",
        "  encoded_data = tokenizer.encode_plus(text=seq_a, text_pair=seq_b, \n",
        "                                       max_length=MAX_SEQ_LEN, pad_to_max_length=True)\n",
        "  input_ids.append(encoded_data['input_ids'])\n",
        "  token_type_ids.append(encoded_data['token_type_ids'])\n",
        "  attention_masks.append(encoded_data['attention_mask'])\n",
        "\n",
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
        "    input_ids, labels, random_state=RANDOM_SEED, test_size=0.25)\n",
        "\n",
        "# # Use train_test_split to split our data into train and validation sets for training\n",
        "# train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(train_val_inputs, train_val_labels, \n",
        "#                                                             random_state=1, test_size=0.1)\n",
        "\n",
        "train_token_types, validation_token_types, _, _ = train_test_split(token_type_ids, input_ids,\n",
        "                                             random_state=1, test_size=0.25)\n",
        "\n",
        "train_masks, validation_masks, _, _ = train_test_split(\n",
        "    attention_masks, input_ids, random_state=RANDOM_SEED, test_size=.25)\n",
        "\n",
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_token_types = torch.tensor(train_token_types)\n",
        "validation_token_types = torch.tensor(validation_token_types)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "# Select a batch size for training. \n",
        "batch_size = BATCH_SIZE\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader \n",
        "train_data = TensorDataset(train_inputs, train_token_types, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_token_types, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASAjpwWFPsUC",
        "colab_type": "text"
      },
      "source": [
        "### Fine-tuning model and validation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuCuGGNodSTM",
        "colab_type": "code",
        "outputId": "af85fa62-bf7e-4bee-b927-8597c0ad26a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "# specify GPU device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)\n",
        "\n",
        "\n",
        "# BERT fine-tuning parameters\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(params=optimizer_grouped_parameters, lr=2e-5, weight_decay=0.01)\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "  \n",
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "# Number of training epochs \n",
        "epochs = NUM_EPOCHS\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "# BERT training loop\n",
        "for epoch in range(epochs):  \n",
        "  print('Epoch {}/{}'.format(epoch+1, epochs))\n",
        "  \n",
        "  ## TRAINING\n",
        "  \n",
        "  # Set our model to training mode\n",
        "  model.train()  \n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_token_type_ids, b_input_mask, b_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    loss, _ = model(input_ids = b_input_ids, token_type_ids=b_token_type_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "    # loss, _ = model(input_ids = b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "  print(\"Train loss: {:.4f}\".format(tr_loss/nb_tr_steps))\n",
        "       \n",
        "  ## VALIDATION\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_token_type_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=b_token_type_ids, attention_mask=b_input_mask) \n",
        "      # logits = model(b_input_ids, attention_mask=b_input_mask)    \n",
        "    # Move logits and labels to CPU\n",
        "    logits = np.array(logits[0].cpu())\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "  print(\"Validation Accuracy: {:.4f}\\n\".format(eval_accuracy/nb_eval_steps))\n",
        "\n",
        "# plot training performance\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(train_loss_set)\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "Train loss: 0.4876\n",
            "Validation Accuracy: 0.7840\n",
            "\n",
            "Epoch 2/4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e57f728181ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mtrain_loss_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Update parameters and take a step using the computed gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab-pa4ROGLdc",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eY4TiADtQlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, test_inputs, token_types, test_masks, test_labels):\n",
        "  # Convert all of our data into torch tensors, the required datatype for our model\n",
        "  prediction_inputs = torch.tensor(test_inputs)\n",
        "  prediction_token_types = torch.tensor(token_types)\n",
        "  prediction_masks = torch.tensor(test_masks)\n",
        "  prediction_labels = torch.tensor(test_labels)\n",
        "\n",
        "  # Select a batch size for training. \n",
        "  batch_size = 32\n",
        "\n",
        "  # Create an iterator of our data with torch DataLoader \n",
        "  prediction_data = TensorDataset(prediction_inputs, prediction_token_types, prediction_masks, prediction_labels)\n",
        "  prediction_sampler = SequentialSampler(prediction_data)\n",
        "  prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "  ## Prediction on test set\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "  # Predict \n",
        "  for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_token_type_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=b_token_type_ids, attention_mask=b_input_mask)\n",
        "      # logits = model(b_input_ids, attention_mask=b_input_mask)\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits[0].cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()  \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "    \n",
        "  matthews_set = []\n",
        "  for i in range(len(true_labels)):\n",
        "    matthews = matthews_corrcoef(true_labels[i],\n",
        "                  np.argmax(predictions[i], axis=1).flatten())\n",
        "    matthews_set.append(matthews)\n",
        "    \n",
        "  # Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n",
        "  flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "  flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "  flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "  scores = {\n",
        "    'matthews_corrcoef_acc': matthews_corrcoef(flat_true_labels, flat_predictions),\n",
        "    'precision': precision_score(flat_true_labels, flat_predictions),\n",
        "    'recall': recall_score(flat_true_labels, flat_predictions),\n",
        "    'f1_score': f1_score(flat_true_labels, flat_predictions),\n",
        "    'accuracy': accuracy_score(flat_true_labels, flat_predictions)\n",
        "  }\n",
        "\n",
        "  return scores\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrkwRRnzHhsi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "semeval_file_path = DATASET_DIR_PATH + 'semeval-benchmark-v1.csv'\n",
        "risk_file_path = DATASET_DIR_PATH + 'risk-models-benchmark-v1.csv'\n",
        "nato_sfa_file_path = DATASET_DIR_PATH + 'nato-sfa-benchmark-v1.csv'\n",
        "ce_me_file_path = DATASET_DIR_PATH + 'ce-me-benchmark-v1.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kcWa-GPPXEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sem_eval_df = pd.read_csv(semeval_file_path, header=None)\n",
        "sem_eval_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTN9zGfBvJSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "risk_df = pd.read_csv(risk_file_path, header=None)\n",
        "risk_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21WAp3jxvLz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "nato_sfa_df = pd.read_csv(nato_sfa_file_path, header=None)\n",
        "nato_sfa_df.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOIiFQXwvOA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ce_me_df = pd.read_csv(ce_me_file_path, header=None)\n",
        "ce_me_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "089SNvuLIzcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
        "\n",
        "dataset_dfs = [sem_eval_df, nato_sfa_df, risk_df, ce_me_df]\n",
        "dataset_names = ['SemEval', 'NATO-SFA', 'Risk Models', 'CE Pairs']\n",
        "print('Dataset, accuracy, Precision, Recall, F1-score')\n",
        "\n",
        "for dataset_df, dataset_name in zip(dataset_dfs, dataset_names):\n",
        "  seq_a_list = dataset_df[0].tolist()\n",
        "  seq_b_list = dataset_df[1].tolist()\n",
        "  labels = [1 if label=='causal' else 0 for label in dataset_df[2].tolist()]\n",
        "\n",
        "  input_ids = []\n",
        "  token_type_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  for seq_a, seq_b in zip(seq_a_list, seq_b_list):\n",
        "    encoded_data = tokenizer.encode_plus(text=seq_a, text_pair=seq_b, max_length=10, pad_to_max_length=True)\n",
        "    input_ids.append(encoded_data['input_ids'])\n",
        "    token_type_ids.append(encoded_data['token_type_ids'])\n",
        "    attention_masks.append(encoded_data['attention_mask'])\n",
        "\n",
        "  scores = evaluate(model, input_ids, token_type_ids, attention_masks, labels)\n",
        "  print('{}, {:.4f}, {:.4f}, {:.4f}, {:.4f}'.format(dataset_name, scores['accuracy'], scores['precision'], scores['recall'], scores['f1_score']))\n",
        "\n",
        "  # print('Classification accuracy of dataset {0} is {1:0.2%}'.format(dataset_name, scores['matthews_corrcoef_acc']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}